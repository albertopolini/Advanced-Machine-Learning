{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Multi-layer Fully Connected Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertopolini/Advanced-Machine-Learning/blob/main/1_Multi_layer_Fully_Connected_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5KnKFU8LAtC"
      },
      "source": [
        "<img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" width=\"20%\" />\n",
        "\n",
        "## Keras: Deep Learning library for Theano and TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0F3172rLAtE"
      },
      "source": [
        ">Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. \n",
        "\n",
        ">It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
        "ref: https://keras.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRQSCfdaLAtF"
      },
      "source": [
        "<a name=\"kaggle\"></a>\n",
        "### Kaggle Challenge Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbeKfwkVLAtG"
      },
      "source": [
        ">The Otto Group is one of the worldâ€™s biggest e-commerce companies, A consistent analysis of the performance of products is crucial. However, due to diverse global infrastructure, many identical products get classified differently.\n",
        "For this competition, we have provided a dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories. \n",
        "Each row corresponds to a single product. There are a total of 93 numerical features, which represent counts of different events. All features have been obfuscated and will not be defined any further.\n",
        "\n",
        "https://www.kaggle.com/c/otto-group-product-classification-challenge/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFq7n6UHLAtJ"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "This algorithm allows us to solve problems of classification (supervised learning). \n",
        "\n",
        "In fact, to estimate the dependent variable, now we make use of the so-called **logistic function** or **sigmoid**. \n",
        "\n",
        "It is precisely because of this feature we call this algorithm logistic regression.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png\" width=\"50%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffeZ8QrxLAtK"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdAVJQYmMKtt"
      },
      "source": [
        "## Utility functions\n",
        "\n",
        "Utility functions to load Kaggle Otto Group Challenge Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0GV97-fMI4B"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "def load_data(path, train=True):\n",
        "    \"\"\"Load data from a CSV File\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    path: str\n",
        "        The path to the CSV file\n",
        "        \n",
        "    train: bool (default True)\n",
        "        Decide whether or not data are *training data*.\n",
        "        If True, some random shuffling is applied.\n",
        "        \n",
        "    Return\n",
        "    ------\n",
        "    X: numpy.ndarray \n",
        "        The data as a multi dimensional array of floats\n",
        "    ids: numpy.ndarray\n",
        "        A vector of ids for each sample\n",
        "    \"\"\"\n",
        "    text = pd.read_csv(path, encoding = \"ISO-8859-2\")\n",
        "    df = pd.read_csv(path)\n",
        "    X = df.values.copy()\n",
        "    if train:\n",
        "        np.random.shuffle(X)  \n",
        "        X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
        "        return X, labels\n",
        "    else:\n",
        "        X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
        "        return X, ids\n",
        "        \n",
        "        \n",
        "def preprocess_data(X, scaler=None):\n",
        "    \"\"\"Preprocess input data by standardise features \n",
        "    by removing the mean and scaling to unit variance\"\"\"\n",
        "    if not scaler:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    return X, scaler\n",
        "\n",
        "\n",
        "def preprocess_labels(labels, encoder=None, categorical=True):\n",
        "    \"\"\"Encode labels with values among 0 and `n-classes-1`\"\"\"\n",
        "    if not encoder:\n",
        "        encoder = LabelEncoder()\n",
        "        encoder.fit(labels)\n",
        "    y = encoder.transform(labels).astype(np.int32)\n",
        "    if categorical:\n",
        "        y = np_utils.to_categorical(y)\n",
        "    return y, encoder"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBIzMqkWLAtL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaGr3p-HmvNU"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcwhGx_rLAtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb0601c-7510-4a8b-8e0b-6c67a7e019db"
      },
      "source": [
        "url_train = 'https://raw.githubusercontent.com/leriomaggio/deep-learning-keras-tensorflow/master/data/kaggle_ottogroup/train.csv'\n",
        "url_test = 'https://raw.githubusercontent.com/leriomaggio/deep-learning-keras-tensorflow/master/data/kaggle_ottogroup/test.csv'\n",
        "X_train, labels = load_data(url_train, train=True)\n",
        "\n",
        "print(\"Training set data\")\n",
        "print(X_train.shape)\n",
        "\n",
        "print(\"Training set labels\")\n",
        "print(labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set data\n",
            "(61878, 93)\n",
            "Training set labels\n",
            "['Class_6' 'Class_2' 'Class_3' ... 'Class_2' 'Class_3' 'Class_6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHG7PflKmzeL"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4huHaAtLAta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e358692-2cc2-49a5-cb41-092e7f397317"
      },
      "source": [
        "X_train, labels = load_data(url_train, train=True)\n",
        "X_train, scaler = preprocess_data(X_train)\n",
        "Y_train, encoder = preprocess_labels(labels)\n",
        "\n",
        "X_test, ids = load_data(url_test, train=False)\n",
        "X_test, _ = preprocess_data(X_test, scaler)\n",
        "\n",
        "nb_classes = Y_train.shape[1]\n",
        "print(nb_classes, 'classes')\n",
        "\n",
        "dims = X_train.shape[1]\n",
        "print(dims, 'dims')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 classes\n",
            "93 dims\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl6vFpY4LAtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438efd90-f2ac-4564-a234-7edae1b2da7b"
      },
      "source": [
        "help(preprocess_data)\n",
        "help(preprocess_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function preprocess_data in module __main__:\n",
            "\n",
            "preprocess_data(X, scaler=None)\n",
            "    Preprocess input data by standardise features \n",
            "    by removing the mean and scaling to unit variance\n",
            "\n",
            "Help on function preprocess_labels in module __main__:\n",
            "\n",
            "preprocess_labels(labels, encoder=None, categorical=True)\n",
            "    Encode labels with values among 0 and `n-classes-1`\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIQdg3xYLAtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83531449-44ef-45cb-d42d-2bb3c7e072d8"
      },
      "source": [
        "np.unique(labels)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6',\n",
              "       'Class_7', 'Class_8', 'Class_9'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr8D9bzfLAty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c7cd39-6e68-4c30-ab25-7634db1f43e2"
      },
      "source": [
        "Y_train  # one-hot encoding"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwBGxwW0LAt4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcLqsyXhLAt5"
      },
      "source": [
        "# Using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQEVijri2CXu"
      },
      "source": [
        "from keras.models import Sequential, Input, Model\n",
        "from keras.layers import Dense, Activation"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsSNGNdPLAt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a40641-f920-4c35-8205-540b17238699"
      },
      "source": [
        "#sequential api\n",
        "dims = X_train.shape[1]\n",
        "print(dims, 'dims')\n",
        "print(\"Building model...\")\n",
        "\n",
        "nb_classes = Y_train.shape[1]\n",
        "print(nb_classes, 'classes')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(nb_classes, input_shape=(dims,), activation='gelu'))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "model.fit(X_train, Y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93 dims\n",
            "Building model...\n",
            "9 classes\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 9)                 846       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 9)                 0         \n",
            "=================================================================\n",
            "Total params: 846\n",
            "Trainable params: 846\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1934/1934 [==============================] - 3s 1ms/step - loss: 1.0585\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79db1c32d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrIvyTBULAuE"
      },
      "source": [
        "Simplicity is pretty impressive right? :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jlBY1QlLAuF"
      },
      "source": [
        "Now lets understand:\n",
        "<pre>The core data structure of Keras is a <b>model</b>, a way to organize layers. The main type of model is the <b>Sequential</b> model, a linear stack of layers.</pre>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFP1kVTwLAuH"
      },
      "source": [
        "What we did here is stacking a Fully Connected (<b>Dense</b>) layer of trainable weights from the input to the output and an <b>Activation</b> layer on top of the weights layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k1wo6Zq2Mnc",
        "outputId": "f426b278-1dbf-47e4-cf16-df0385ddd10c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# \"Model\" api\n",
        "dims = X_train.shape[1]\n",
        "print(dims, 'dims')\n",
        "print(\"Building model...\")\n",
        "\n",
        "nb_classes = Y_train.shape[1]\n",
        "print(nb_classes, 'classes')\n",
        "\n",
        "inputs = Input(shape=(dims,))\n",
        "x = Dense(nb_classes, activation='sigmoid')(inputs)\n",
        "output = Activation('softmax')(x)\n",
        "model = Model(inputs, output)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
        "model.fit(X_train, Y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93 dims\n",
            "Building model...\n",
            "9 classes\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 93)]              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 9)                 846       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 9)                 0         \n",
            "=================================================================\n",
            "Total params: 846\n",
            "Trainable params: 846\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1934/1934 [==============================] - 2s 1ms/step - loss: 1.9939\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79d5ce28d0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoiloE1G2YDX"
      },
      "source": [
        ">The **Model** groups layers into an object with training and inference features. In Functional model,\n",
        "part or all of the inputs directly connected to the output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAz-flsELAuO"
      },
      "source": [
        "#### Dense Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "iIMtJlebLAuP"
      },
      "source": [
        "```python\n",
        "from keras.layers.core import Dense\n",
        "\n",
        "Dense(units, activation=None, use_bias=True, \n",
        "      kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
        "      kernel_regularizer=None, bias_regularizer=None, \n",
        "      activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqdK4XsTLAuR"
      },
      "source": [
        "* `units`: int > 0.\n",
        "\n",
        "* `init`: name of initialization function for the weights of the layer (see initializations), or alternatively, Theano function to use for weights initialization. This parameter is only relevant if you don't pass a weights argument.\n",
        "\n",
        "* `activation`: name of activation function to use (see activations), or alternatively, elementwise Theano function. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "\n",
        "* `weights`: list of Numpy arrays to set as initial weights. The list should have 2 elements, of shape (input_dim, output_dim) and (output_dim,) for weights and biases respectively.\n",
        "\n",
        "* `kernel_regularizer`: instance of WeightRegularizer (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
        "\n",
        "* `bias_regularizer`: instance of WeightRegularizer, applied to the bias.\n",
        "\n",
        "* `activity_regularizer`: instance of ActivityRegularizer, applied to the network output.\n",
        "\n",
        "* `kernel_constraint`: instance of the constraints module (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
        "\n",
        "* `bias_constraint`: instance of the constraints module, applied to the bias.\n",
        "\n",
        "* `use_bias`: whether to include a bias (i.e. make the layer affine rather than linear)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8HLgiIpLAuS"
      },
      "source": [
        "## (some) others `keras.core.layers`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kPjIvGXLAuV"
      },
      "source": [
        "* `keras.layers.core.Flatten()`\n",
        "* `keras.layers.core.Reshape(target_shape)`\n",
        "* `keras.layers.core.Permute(dims)`\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Permute((2, 1), input_shape=(10, 64)))\n",
        "# now: model.output_shape == (None, 64, 10)\n",
        "# note: `None` is the batch dimension\n",
        "```\n",
        "\n",
        "* `keras.layers.core.Lambda(function, output_shape=None, arguments=None)`\n",
        "* `keras.layers.core.ActivityRegularization(l1=0.0, l2=0.0)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OY6pTKPLAuZ"
      },
      "source": [
        "<img src=\"https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/imgs/dl_overview.png?raw=true\" >\n",
        "\n",
        "Credits: Yam Peleg ([@Yampeleg](https://twitter.com/yampeleg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxGukwnhLAub"
      },
      "source": [
        "##### Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ySBf_NHLLAud"
      },
      "source": [
        "```python\n",
        "from keras.layers.core import Activation\n",
        "\n",
        "Activation(activation)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26hx3_ozLAue"
      },
      "source": [
        "**Supported Activations** : softmax, elu, relu, tanh, sigmoid, linear, ... https://keras.io/api/layers/activations\n",
        "\n",
        "**Advanced Activations**: https://keras.io/api/layers/activations/#about-advanced-activation-layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOXIN-UfLAvC"
      },
      "source": [
        "# Multi-Layer Fully Connected Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UejT5THFLAvD"
      },
      "source": [
        "<img src=\"https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/imgs/MLP.png?raw=true\" width=\"45%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM6HQ54TLAvD"
      },
      "source": [
        "#### Forward and Backward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCTkRK-vLAvF"
      },
      "source": [
        "<img src=\"https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/imgs/backprop.png?raw=true\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRWqUpr4LAvG"
      },
      "source": [
        "**Q:** _How hard can it be to build a Multi-Layer Fully-Connected Network with keras?_\n",
        "\n",
        "**A:** _It is basically the same, just add more layers!_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohXiMoIoLAvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441c0d2a-ef46-434a-cb78-cc8e059f5068"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1000, input_shape=(dims,), activation = \"relu\"))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 1000)              94000     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 9)                 9009      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 9)                 0         \n",
            "=================================================================\n",
            "Total params: 103,009\n",
            "Trainable params: 103,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFkv3fYsLAvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0312a523-e53c-4c7a-ae12-c82fa72f178a"
      },
      "source": [
        "model.fit(X_train, Y_train, epochs=20, \n",
        "          batch_size=128, verbose=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 1.1947\n",
            "Epoch 2/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.8316\n",
            "Epoch 3/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.7505\n",
            "Epoch 4/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.7094\n",
            "Epoch 5/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.6832\n",
            "Epoch 6/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.6643\n",
            "Epoch 7/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.6496\n",
            "Epoch 8/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.6377\n",
            "Epoch 9/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.6277\n",
            "Epoch 10/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.6189\n",
            "Epoch 11/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.6114\n",
            "Epoch 12/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.6046\n",
            "Epoch 13/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.5985\n",
            "Epoch 14/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.5930\n",
            "Epoch 15/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.5880\n",
            "Epoch 16/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.5832\n",
            "Epoch 17/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.5788\n",
            "Epoch 18/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.5746\n",
            "Epoch 19/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.5708\n",
            "Epoch 20/20\n",
            "484/484 [==============================] - 2s 4ms/step - loss: 0.5671\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79d741fb50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ7zEcxHLAvQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvslXL1GLAvR"
      },
      "source": [
        "# Your Turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOwHDEC7LAvS"
      },
      "source": [
        "## Hands On - Keras Fully Connected\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6lknIm0LAvT"
      },
      "source": [
        "Take couple of minutes and try to play with the number of layers and the number of parameters in the layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2tG8KbGLAvU",
        "outputId": "b4855ad0-4aa3-494d-8068-d225c7766384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(100, input_shape=(dims,)))\n",
        "\n",
        "model.add(Dense(80, input_shape=(dims,)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(50, input_shape=(dims,)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(20, input_shape=(dims,)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# FC@80 -> FC@50\n",
        "\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_59 (Dense)             (None, 100)               9400      \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 80)                8080      \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 50)                4050      \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 20)                1020      \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 9)                 189       \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 9)                 0         \n",
            "=================================================================\n",
            "Total params: 22,739\n",
            "Trainable params: 22,739\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeAwUpm0LAvZ",
        "outputId": "41f45eef-8f27-49e7-fd71-781116d071e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(X_train, Y_train, epochs=20, batch_size=128, verbose=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "484/484 [==============================] - 2s 2ms/step - loss: 0.7505\n",
            "Epoch 2/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.5768\n",
            "Epoch 3/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.5464\n",
            "Epoch 4/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.5290\n",
            "Epoch 5/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.5152\n",
            "Epoch 6/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.5048\n",
            "Epoch 7/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4957\n",
            "Epoch 8/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4852\n",
            "Epoch 9/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4808\n",
            "Epoch 10/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4722\n",
            "Epoch 11/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4645\n",
            "Epoch 12/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4606\n",
            "Epoch 13/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4544\n",
            "Epoch 14/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4493\n",
            "Epoch 15/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4457\n",
            "Epoch 16/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4400\n",
            "Epoch 17/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4368\n",
            "Epoch 18/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4324\n",
            "Epoch 19/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4288\n",
            "Epoch 20/20\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.4255\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79cf7d9310>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4HfCTYXLAve"
      },
      "source": [
        "Building a question answering system, an image classification model, a Neural Turing Machine, a word2vec embedder or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?"
      ]
    }
  ]
}